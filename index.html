<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Emergent open-vocabulary semantic segmentation from off-the-shelf vision-language models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://sites.google.com/view/jiayunluo/jiayun-luo-letitia" target="_blank">Jiayun Luo</a><sup>†</sup><sup>‡</sup>,</span>
                <span class="author-block">
                  <a href="https://siddheshk.github.io/" target="_blank">Siddhesh Khandelwal</a><sup>‡</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.ubc.ca/~lsigal/" target="_blank">Leonid Sigal</a><sup>‡</sup>,</span>
                  <span class="author-block">
                    <a href="http://boyangli.org/" target="_blank">Boyang Li</a><sup>†</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Nanyang Technological University, Singapore<sup>†</sup><br>
                        University of British Columbia, Vector Institute for AI, Canada<sup>‡</sup><br>CVPR 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2311.17095" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Luo_Emergent_Open-Vocabulary_Semantic_CVPR_2024_supplemental.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/letitiabanana/PnP-OVSS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.17095" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/PnPOVSS_projectpage.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
         
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            From image-text pairs, large-scale vision-language models (VLMs) learn to implicitly associate image regions with words, which prove effective for tasks like visual question answering. However, leveraging the learned association for open-vocabulary semantic segmentation remains a challenge. In this paper, we propose a simple, yet extremely effective, training-free technique, Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss. To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask. PnP-OVSS does not require any neural network training and performs hyperparameter tuning without the need for any segmentation annotations, even for a validation set. PnP-OVSS demonstrates substantial improvements over comparable baselines (+26.2% mIoU on Pascal VOC, +20.5% mIoU on MS COCO, +3.1% mIoU on COCO Stuff and  +3.0% mIoU on ADE20K).  
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Single image section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Section Title -->
      <h2 class="title is-3">Over-segmentation and Under-segmentation</h2>
      <img src="static/images/PnP-OVSS_Teaser.png" alt="HIST quali"
           style="max-width: 60%; height: auto; display: block; margin: 0 auto;" />
      <h2 class="subtitle" style="margin-top: 1.5rem;">
        The naive cross-attention masks are too inclusive (Over-segment) whereas GradCAM is too exclusive (Under-segment). Therefore we proposed to combined Salience Dropout with Gradcam to extract segmentation masks from off-the-shelf VLM for open-set objects.
  </div>
</section>



<!-- Single image section -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Section Title -->
      <h2 class="title is-3">Salience Dropout</h2>
      <img src="static/images/PNP-OVSS_saliencedrop.png" alt="HIST quali"
           style="max-width: 20%; height: auto; display: block; margin: 0 auto;" />
      <h2 class="subtitle" style="margin-top: 1.5rem;">
        Salience DropOut is an iterative process designed to encourage Vision-Language Models (VLMs) to attend to less discriminative image regions often overlooked by GradCAM. In each iteration, it computes a class-agnostic salience map by summing GradCAM salience maps across predicted classes of the image, then zeros out the top 10 most salient patches. This process is repeated for three iterations, ensuring previously masked regions remain excluded. The final salience map for each class is obtained by summing salience maps from all iterations.
  </div>
</section>

<!-- Single image section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Section Title -->
      <h2 class="title is-3">Automatic Dense-label-free Hyperparameter Tuning</h2>
      <img src="static/images/PnP-OVSS_autotune.jpg" alt="HIST quali"
           style="max-width: 60%; height: auto; display: block; margin: 0 auto;" />
      <h2 class="subtitle" style="margin-top: 1.5rem;">
        To tune the key hyperparameters in PnP-OVSS—namely the cross-attention layer 𝐿, attention head H, and binary threshold T—we propose a weakly supervised reward function that avoids the need for pixel-level labels. Instead, the method only requires knowledge of which classes appear in each image. For each class in an image, the predicted segmentation mask is applied, and the masked image is fed into a pretrained network along with the class name to compute a similarity score. A reward is given if the masked image is more similar to the correct class name than to a black image. These rewards are summed across a validation set to guide hyperparameter search. The final parameters, including the Gaussian blur variance, are selected using simple random search.
  </div>
</section>

  
<!-- Single image section -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Section Title -->
      <h2 class="title is-3">Qualitative examples</h2>
      <img src="static/images/PNP-OVSS_quali.png" alt="HIST quali"
           style="max-width: 60%; height: auto; display: block; margin: 0 auto;" />
      <h2 class="subtitle" style="margin-top: 1.5rem;">
        Qualitative Results of PnP-OVSS + BLIP. Images are from Pascal VOC and COCO Object. The right columns and bottom rows show the ground-truth (GT); the rest are our results. Note the good results on small objects like the frisbee and the tennis racket.
  </div>
</section>

  

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
     <!-- Section Title -->
      <h2 class="title is-3">In the wild examples</h2>
      <video poster="" id="tree" autoplay controls muted loop style="max-width: 80%; height: auto; display: block; margin: 0 auto;">
        <!-- Your video here -->
        <source src="static/videos/PnP-OVSS.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->





<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/12308_PnP_OVSS_Poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{luo2024emergentopenvocabularysemanticsegmentation,
      title={Emergent Open-Vocabulary Semantic Segmentation from Off-the-shelf Vision-Language Models}, 
      author={Jiayun Luo and Siddhesh Khandelwal and Leonid Sigal and Boyang Li},
      year={2024},
      eprint={2311.17095},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17095},}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
